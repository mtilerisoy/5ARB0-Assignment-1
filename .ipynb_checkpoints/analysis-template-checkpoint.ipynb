{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis Methods\n",
    "## \\[5ARB0\\] Data Acquisition and Analysis - Technical Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell imports the libraries or packages that you can use during this assignment\n",
    "# you are not allowed to import additional libraries or packages\n",
    "from helpers import *\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Important**\n",
    ">\n",
    "> Do not import any other packages or libraries than the ones already provided to you. You can only use the imported packages _after_ they have been imported.\n",
    ">\n",
    "> Write your code between the `BEGIN_TODO` and `END_TODO` markers. Do not change these markers.\n",
    ">\n",
    "> Restart your notebook and run all cells before submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this assignment you will learn about different data analysis methods for creating a fall detector. \n",
    "\n",
    "This assignment is split into 4 parts. Part 1 will focus on principal component analysis and part 2 will introduce independent component analysis. Part 3 will guide you through implementing your own K-means algorithm. Finally, part 4 will introduce you to the Gaussian mixture model, which you will train using the expectation-maximization algorithm. In the next assignment, you will work on using these methods for creating a fall detector.\n",
    "\n",
    "\n",
    "### Learning goals\n",
    "After this assignment you can\n",
    "- implement the PCA algorithm;\n",
    "- determine a suitable number of PCA components;\n",
    "- use PCA for image compression;\n",
    "- use the ICA algorithm;\n",
    "- explain differences between PCA and ICA;\n",
    "- implement the K-means algorithm;\n",
    "- explain issues and shortcoming of the K-means algorithm;\n",
    "- implement the Gaussian mixture model for clustering;\n",
    "- explain how the Gaussian mixture model differs from the K-means algorithm;\n",
    "- apply both algorithms on a real data set;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Principal component analysis\n",
    "\n",
    "Recorded data is often high-dimensional, leading to a large computational burden and limiting real-time data processing. Quite often we would therefore like to compress the data, such that it uses less memory. The creation of .zip-files on your computer is an example of data compression. In this part we will look at one method that achieves data compression, namely Principal component analysis (PCA). PCA is a useful technique for analyzing high-dimensional data, and compressing this data for storage or processing purposes. PCA aims to fit a orthogonal coordinate system to a dataset that best captures the variance or spread of the data. This approach is very closely related to the eigendecomposition of a matrix.\n",
    "\n",
    "PCA aims to find the orthogonal directions of most variance of the dataset. Therefore it first computes the covariance matrix of the dataset, which can be estimated as\n",
    "$$\\Sigma = \\mathrm{Cov}[{\\bf{x}}] \\approx \\frac{1}{N-1} \\sum_{n=1}^N ({\\bf{x}}_n - \\mathrm{E}[{\\bf{x}}])({\\bf{x}}_n - \\mathrm{E}[{\\bf{x}}])^\\top,$$\n",
    "where ${\\bf{x}}_n$ is the $n$-th data sample and where $\\mathrm{E}[{\\bf{x}}]$ represents the expected value, or mean, which can be estimated as\n",
    "$$\\mathrm{E}[{\\bf{x}}] \\approx \\frac{1}{N} \\sum_{n=1}^N {\\bf{x}}_n.$$\n",
    "The covariance matrix captures the variances of the individual elements/features in ${\\bf{x}}_n$ and the covariances between elements/features. \n",
    "\n",
    "Based on the obtained covariance matrix, you will perform an eigendecomposition. An eigenvalue decomposition finds a set of eigenvectors and corresponding eigenvalues. An eigenvector ${\\bf{v}}$ and corresponding eigenvalue ${\\lambda}$ of some square matrix $A$ satisfy the equation\n",
    "$$A {\\bf{v}} = \\lambda {\\bf{v}}.$$\n",
    "In matrix notation, this can be written as\n",
    "$$A Q = Q\\Lambda,$$\n",
    "where $Q$ is a orthonormal matrix where each column represents an eigenvector. Orthonormal refers to the individual eigenvectors being of unit length, and perpendicular to eachother. This matrix satisfies the useful properties $QQ^\\top = Q^\\top Q = I$ and $Q^\\top = Q^{-1}$. $\\Lambda$ is a diagonal matrix, whose diagonal contains the eigenvalues of $A$. The matrix $A$ can therefore be decomposed as \n",
    "$$A = Q\\Lambda Q^\\top$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 1.1: Compute eigenvalues and eigenvectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `compute_eigen(X)` which computes the eigenvalues and eigenvectors of the covariance matrix of some dataset `X`. The dataset is a matrix of shape ($N\\times M$), where $N$ denotes the number of data samples and $M$ the dimensionality of the features. The function should return a vector of length $M$ containing the eigenvalues and a matrix of shape ($M\\times M$) containing the corresponding eigenvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [5ARB0_Analysis_1_1a] Compute eigenvalues and eigenvectors\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [5ARB0_Analysis_1_1a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data\n",
    "X = ex11_generate_data()\n",
    "\n",
    "# compute eigenvalues and eigenvectors\n",
    "eigvals, eigvecs = compute_eigen(X)\n",
    "\n",
    "# plot data\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(X[:,0], X[:,1], alpha=0.2)\n",
    "plot_eigen(np.mean(X, axis=0), eigvals, eigvecs, plt.gca(), width=0.1, color=\"r\")\n",
    "plt.axis(\"equal\")\n",
    "plt.grid()\n",
    "\n",
    "# print eigenvalues and eigenvectors\n",
    "eigvals, eigvecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the computed eigenvalues, eigenvectors and the visualization and answer the questions below:\n",
    "- In what direction does the first eigenvector point?\n",
    "- How can the eigenvalue be interpreted?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`#// BEGIN_TODO [5ARB0_Analysis_1_1b] Interpretation of eigenvalues and eigenvectors`\n",
    "\n",
    "<div class='alert alert-warning' role='alert'>Replace this line by your text.</div>\n",
    "\n",
    "`#// END_TODO [5ARB0_Analysis_1_1b]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 1.1\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the computed eigendecomposition, the original dataset can be transformed such that its new mean coincides with the origin (0,0) and that the new eigenvectors specify Euclidean standard basis vectors (i.e. the new covariance matrix is diagonal). The first step can be obtained by subtracting the mean from the dataset. The origin-centered dataset $\\tilde{X}$ now is centered in the origin and therefore the covariance matrix can be estimated as \n",
    "$$ \\tilde{\\Sigma} = \\mathrm{Cov}[\\tilde{\\bf{x}}] \\approx \\frac{1}{N-1} \\tilde{X}^\\top \\tilde{X}\\qquad\\qquad \\text{if }\\mathrm{E}[\\tilde{{\\bf{x}}}] = {\\bf{0}}$$\n",
    "with the corresponding eigendecomposition $\\tilde{\\Sigma} = \\tilde{Q} \\tilde{\\Lambda} \\tilde{Q}^\\top$.\n",
    "\n",
    "If we multiply $\\tilde{X}$ with $\\tilde{Q}$ to get the transformed dataset $Y=\\tilde{X}\\tilde{Q}$, we observe that the new covariance matrix becomes\n",
    "$$\\mathrm{Cov}[{\\bf{y}}] \\approx \\frac{1}{N-1} Y^\\top Y = \\frac{1}{N-1}\\tilde{Q}^\\top \\tilde{X}^\\top \\tilde{X} \\tilde{Q} = \\tilde{Q}^\\top\\tilde{\\Sigma}\\tilde{Q} = \\tilde{Q}^\\top\\tilde{Q} \\tilde{\\Lambda} \\tilde{Q}^\\top\\tilde{Q} = \\tilde{\\Lambda},$$\n",
    "which is diagonal.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 1.2: Data transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `transform_PCA(X, mean, eigvecs)` which translates some dataset `X` to be centered in the origin, and rotates it, such that its new covariance matrix is diagonal. $X$ is of shape ($N\\times M$), where $N$ denotes the number of data samples and $M$ the dimensionality of the features. The function should return the transformed dataset of shape ($N\\times M$). Also create the function `inversetransform_PCA(X, mean, eigvecs)` which performs the inverse transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [5ARB0_Analysis_1_2a] Transform data to PCA space\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [5ARB0_Analysis_1_2a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [5ARB0_Analysis_1_2b] Transform data back from PCA space\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [5ARB0_Analysis_1_2b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform data\n",
    "m = np.mean(X, axis=0)\n",
    "eigvals, eigvecs = compute_eigen(X)\n",
    "Y = transform_PCA(X, m, eigvecs)\n",
    "Z = inversetransform_PCA(Y, m, eigvecs)\n",
    "\n",
    "# plot transformed data\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(10,5))\n",
    "ax[0].scatter(Y[:,0], Y[:,1], alpha=0.2)\n",
    "eigvalsY, eigvecsY = compute_eigen(Y)\n",
    "plot_eigen(np.mean(Y, axis=0), eigvalsY, eigvecsY, ax[0], width=0.1, color=\"r\")\n",
    "ax[1].scatter(Z[:,0], Z[:,1], alpha=0.2)\n",
    "eigvalsZ, eigvecsZ = compute_eigen(Z)\n",
    "plot_eigen(np.mean(Z, axis=0), eigvalsZ, eigvecsZ, ax[1], width=0.1, color=\"r\")\n",
    "ax[0].axis(\"equal\"), ax[1].axis(\"equal\")\n",
    "ax[0].grid(), ax[1].grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 1.2\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far PCA has only been discussed for a toy example. Let's now apply it to high-dimensional data. We will load a dataset containing 400 images of faces. These grayscale images are of size (64 $\\times$ 64) and therefore contain 4096 pixels. In order to process the images, they have been flattened into vectors, which are appended to create a matrix containing 400 images. Below we have plotted the first 100 images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ex13_generate_data()\n",
    "plot_faces(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 1.3: Principal components analysis\n",
    "Compute the eigenvalues and vectors of the faces dataset. Plot the first 350 eigenvalues with both a normal as log-scaling on the y-axis.\n",
    "\n",
    "> Note: Since the dataset only contains 400 images, the covariance matrix of size (4096 $\\times$ 4096) is not positive definite (although it should be in theory). Therefore the eigenvalues > 400 are basically useless, however, they are still computed as imaginary quantities. You can plot the real or absolute values of the eigenvalues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [5ARB0_Analysis_1_3a] Plot eigenvalues\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [5ARB0_Analysis_1_3a]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the created plots, what do you observe? How could this be useful for data compression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`#// BEGIN_TODO [5ARB0_Analysis_1_3b] What do you observe?`\n",
    "\n",
    "<div class='alert alert-warning' role='alert'>Replace this line by your text.</div>\n",
    "\n",
    "`#// END_TODO [5ARB0_Analysis_1_3b]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 1.3\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use PCA for data compression. Instead of performing the transform to the PCA space with the entire eigenvector matrix with shape (4096 $\\times$ 4096), we only use the $K$ eigenvectors corresponding with the $K$ largest eigenvalues. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 1.4: Data compression\n",
    "Compress the faces dataset using PCA in a matrix of shape (400 $\\times$ $K$) and then decompress the data and plot the faces using the `plot_faces()` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [5ARB0_Analysis_1_4a] Data compression\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [5ARB0_Analysis_1_4a]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the results. What do you observe if you change $K$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`#// BEGIN_TODO [5ARB0_Analysis_1_4b] What do you observe?`\n",
    "\n",
    "<div class='alert alert-warning' role='alert'>Replace this line by your text.</div>\n",
    "\n",
    "`#// END_TODO [5ARB0_Analysis_1_4b]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 1.4\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Independent component analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach of finding the most important components in a dataset is independent component analysis (ICA). Below you will use it to analyze a new data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 2.1: Limitations PCA\n",
    "Have a look at the dataset below. Would PCA be a good approach for finding the components of highest variance? Please motivate your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ex21_generate_data()\n",
    "plt.scatter(X[:,0], X[:,1], alpha=0.1)\n",
    "plt.grid(True)\n",
    "plt.axis(\"equal\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`#// BEGIN_TODO [5ARB0_Analysis_2_1] Limitations PCA`\n",
    "\n",
    "<div class='alert alert-warning' role='alert'>Replace this line by your text.</div>\n",
    "\n",
    "`#// END_TODO [5ARB0_Analysis_2_1]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 2.1\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, FastICA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: From this moment onwards you can use the `PCA` and `FastICA` functions from `sklearn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 2.2: PCA versus ICA\n",
    "Use the `PCA` and `FastICA` functions from `sklearn` to create the objects `pca_object` and `ica_object`, each with two components. Fit these objects to the dataset and transform the dataset. Save the transformed dataset into the variables `data_transformed_pca` and `data_transformed_ica`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [5ARB0_Analysis_2_2a] PCA and ICA\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [5ARB0_Analysis_2_2a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(ncols=3, figsize=(15,5))\n",
    "ax[0].scatter(X[:,0], X[:,1], alpha=0.1)\n",
    "ax[1].scatter(data_transformed_pca[:,0], data_transformed_pca[:,1], alpha=0.1)\n",
    "ax[2].scatter(data_transformed_ica[:,0], data_transformed_ica[:,1], alpha=0.1)\n",
    "plot_pca(ax[0], pca_object, np.mean(X, axis=0))\n",
    "plot_ica(ax[0], ica_object, np.mean(X, axis=0))\n",
    "ax[0].grid(True), ax[1].grid(True), ax[2].grid(True)\n",
    "ax[0].axis('equal'), ax[1].axis('equal'), ax[2].axis('equal')\n",
    "ax[0].set_title(\"original data\"), ax[1].set_title(\"transformed data (PCA)\"), ax[2].set_title(\"transformed data (ICA)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run your code a couple of times. What do you observe? Which method works best for this data set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`#// BEGIN_TODO [5ARB0_Analysis_2_2b] PCA versus ICA`\n",
    "\n",
    "<div class='alert alert-warning' role='alert'>Replace this line by your text.</div>\n",
    "\n",
    "`#// END_TODO [5ARB0_Analysis_2_2b]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 2.2\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: K-means algorithm\n",
    "In this part we will discuss unsupervised machine learning problems and describe how the K-means algorithm can be used to solve these.\n",
    "\n",
    "Unsupervised machine learning problems are problems in which we try to determine some particular structure within a data set. On the contrary, supervised machine learning problems require us to model some kind of input-output mapping. Unsupervised machine learning problems do not have a specified output which we would like to model. Instead we are interesting in making sense of the data in grouping this data, without knowing beforehand which and how many groups exist.\n",
    "\n",
    "The K-means algorithm can group $N$ data samples of dimension $D$ into $K$ groups or clusters. These clusters can each be characterized by their mean vector: the expected or average value of the points which are assigned to the cluster. The mean vector denoting the center of the $k^{th}$ cluster can be represented as the column vector ${\\bf{\\mu}}^{(k)} = [\\mu_1^{(k)},\\ \\mu_2^{(k)},\\ \\ldots, \\mu_D^{(k)}]^\\top$ and the $n^{th}$ data sample can be represented by the column vector ${\\bf{x}}^{(n)} = [x_1^{(n)},\\ x_2^{(n)},\\ \\ldots, x_D^{(n)}]^\\top$, where the superscript denotes the sample index.\n",
    "\n",
    "The K-means algorithm tries to minimize the (within-cluster) Euclidean squared distance\n",
    "$$J({\\bf{X}}, {\\bf{\\mu}}) = \\frac{1}{N}\\sum_{n=1}^N \\sum_{k=1}^K \\rho_k^{(n)} \\| {\\bf{x}}^{(n)} - {\\bf{\\mu}}^{(k)}\\|^2$$\n",
    "Here $\\rho_k^{(n)}$ is a so-called indicator function that is defined as \n",
    "$$ \\rho_k^{(n)} = \\begin{cases} 1 & \\text{if sample }{\\bf{x}}^{(n)}\\text{ is assigned to cluster }k \\\\ 0 & \\text{otherwise}\\end{cases}$$\n",
    "This indicator function equals $1$ when the corresponding data point is assigned to the corresponding cluster and $0$ otherwise. The cost function therefore represents the average squared distance with respect to the cluster that a point is assigned to.\n",
    "\n",
    "The algorithm is specified as follows:\n",
    "\n",
    "1. Initialize means ${\\bf{\\mu}}$.\n",
    "2. Assign data points to closest cluster mean (i.e. update $\\rho_k^{(n)}$).\n",
    "3. Calculate new cluster means as the average values of the points that are assigned to it (i.e. update ${\\bf{\\mu}}$).\n",
    "4. Calculate cost function $J({\\bf{X}}, {\\bf{\\mu}})$.\n",
    "5. If not converged, go back to 2 and repeat.\n",
    "\n",
    "Here we will describe the algorithm in words. First the centers of the clusters are initialized. This can be done arbitrarily, but often the centers are set to random (but distinct) samples of the data set.\n",
    "Once the means are set, we assign each data sample to the cluster that is closest to it. In order to do so, we calculate the Euclidean squared distance between a point and all the clusters and find the cluster that is closest to it. We repeat this for all points and we therefore completely specify $\\rho_k^{(n)}$. Once all points have been assigned to a cluster, we look up all points corresponding to a certain cluster and we average these to calculate the new cluster center. We update all cluster means. Then we evaluate the current fit of the clusters on the data by evaluate the cost function. If we still see a significant improvement in the cost function, we repeat updating the assignments and cluster centers and if the cost function seems to have converged, we stop iterating.\n",
    "\n",
    "In this part of the assignment you will implement the K-means algorithm from scratch, starting with the initialization of the cluster means.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 3.1: Initializing cluster centers\n",
    "Consider the function `X = ex3_generate_data()` which generates a matrix ${\\bf{X}}$ of shape (N x D), representing the vertical concatenation of $N$ transposed data vectors of dimension $D$. Create a function `means = initialize_means(X, K)` that accepts the data set ${\\bf{X}}$  and number of clusters $K$ as input and returns a matrix of shape (K x D), representing the vertical concatenation of $K$ transposed mean vectors of dimension $D$. These means should be initialized such that they coincide with *random* samples from the data set, which are always *distinct*. In other words, the means should equal a random subset of the availabe data set, where no means are equal. Also keep in mind that the number of clusters is variable in this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [5ARB0_Analysis_3_1] Complete the function initialize_means(X, K)\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [5ARB0_Analysis_3_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data\n",
    "X = ex3_generate_data()\n",
    "\n",
    "# initialize means\n",
    "means = initialize_means(X, 3)\n",
    "\n",
    "# plot data\n",
    "plt.figure()\n",
    "plt.scatter(X[:,0], X[:,1], 10)\n",
    "plt.scatter(means[:,0], means[:,1], c=\"red\", marker=\"x\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 3.1\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the clusters have been initialized, it is time to assign points to the closest clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 3.2: Assign points to clusters\n",
    "Again consider the function `X = ex4_generate_data()` which generates a matrix ${\\bf{X}}$ of shape (N x D), representing the vertical concatenation of $N$ transposed data vectors of dimension $D$. Create a function `rho = assign_data_to_clusters(X, means)` that accepts the data set ${\\bf{X}}$ and matrix of means ${\\bf{\\mu}}$ as input and returns a matrix of shape (N x K), which contains all indicator functions $\\rho_k^{(n)}$. This matrix should be a matrix of only ones and zeros and each row should sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [5ARB0_Analysis_3_2] Complete the function assign_data_to_clusters(X, means)\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [5ARB0_Analysis_3_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data\n",
    "X = ex3_generate_data()\n",
    "\n",
    "# initialize means\n",
    "means = initialize_means(X, 3)\n",
    "\n",
    "# assign point to clusters\n",
    "rho = assign_data_to_clusters(X, means)\n",
    "\n",
    "# plot data\n",
    "plt.figure()\n",
    "plt.scatter(X[:,0], X[:,1], 10, c=np.argmax(rho, axis=1))\n",
    "plt.scatter(means[:,0], means[:,1], c=\"red\", marker=\"x\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 3.2\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The means have been initialized, the point have been assigned to a cluster. Now the cluster centers can be updated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 3.3: Update cluster centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again consider the function `X = ex3_generate_data()` which generates a matrix ${\\bf{X}}$ of shape (N x D), representing the vertical concatenation of $N$ data vectors of dimension $D$. Create a function `means = update_cluster_centers(X, rho)` that accepts the data set ${\\bf{X}}$ and matrix of indicators $\\rho$ as input and returns a matrix of shape (K x D), which contains the new cluster centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [5ARB0_Analysis_3_3] Complete the function update_cluster_centers(X, rho)\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [5ARB0_Analysis_3_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data\n",
    "X = ex3_generate_data()\n",
    "\n",
    "# initialize means\n",
    "means = initialize_means(X, 3)\n",
    "\n",
    "# assign point to clusters\n",
    "rho = assign_data_to_clusters(X, means)\n",
    "\n",
    "# update means\n",
    "means_new = update_cluster_centers(X, rho)\n",
    "\n",
    "# plot data\n",
    "plt.figure()\n",
    "plt.scatter(X[:,0], X[:,1], 10, c=np.argmax(rho, axis=1))\n",
    "plt.scatter(means[:,0], means[:,1], 50, c=\"red\", marker=\"x\")\n",
    "plt.scatter(means_new[:,0], means_new[:,1], 50, c=\"blue\", marker=\"x\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 3.3\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost there! Now it is just a matter of combining the previous functions for finalizing the K-means algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 3.4: Implement K-means algorithm\n",
    "\n",
    "First create a function `J = Kmeans_loss(X, means, rho)` that calculates the within-cluster Euclidean squared distance as defined above. Secondly create the final `means, rho, J = Kmeans(X, K)` function that combines all previous functions to create the K-means algorithm as specified in the the introduction of this part of the assignment. This function returns the final cluster centers, the indicator function and a history of the losses. Save the loss *after* each iteration and stop iterating when the difference in loss does no longer exceed 1e-10. The initial loss based on the randomly initialized means should not be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [5ARB0_Analysis_3_4a] Complete the Kmeans_loss(X, means, rho) function\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [5ARB0_Analysis_3_4a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [5ARB0_Analysis_3_4b] Complete the Kmeans(X, K) function\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [5ARB0_Analysis_3_4b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data\n",
    "X = ex3_generate_data()\n",
    "\n",
    "# initialize means\n",
    "means, rho, J = Kmeans(X, 3)\n",
    "\n",
    "# plot data\n",
    "_,ax = plt.subplots(ncols=2, figsize=(15,5))\n",
    "ax[0].scatter(X[:,0], X[:,1], 10, c=np.argmax(rho, axis=1))\n",
    "ax[0].scatter(means[:,0], means[:,1], 50, c=\"red\", marker=\"x\")\n",
    "ax[1].plot(J)\n",
    "ax[0].grid(), ax[1].grid(), ax[1].set_ylabel(\"cost function\"), ax[1].set_xlabel(\"iteration\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 3.4\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 3.5: Number of clusters\n",
    "In the previous assignment the data had been generate from 3 clusters. In practice the number of clusters is often unknown. In this exercise we will see what happens when we add extra clusters. In this exercise, run your K-means algorithm on the previous data set for 2 up to and including 10 clusters and save the final value of the loss (i.e. the loss value when the algorithm has converged).\n",
    "\n",
    "Plot the final loss against the number of used clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data\n",
    "X = ex3_generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [5ARB0_Analysis_3_5a] Plot the Kmeans loss against the number of clusters\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [5ARB0_Analysis_3_5a]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the loss give a good impression of how many clusters can be found in the data set?\n",
    "\n",
    "1. Yes, the loss tells us how good the data is represented. As the loss decreases, the model becomes better at representing the data. Therefore the final loss provides a good indication how many clusters are desirable.\n",
    "2. Yes, if we have a very large number of clusters, the loss can be brought to zero, leading to a perfect representation of the data.\n",
    "3. No, the loss function only decreases and does not take into account the complexity introduced by adding more clusters.\n",
    "4. All of the above answers are correct.\n",
    "5. None of the above answers are correct\n",
    "\n",
    "Answer this question by assigning the number corresponding to your answer to the variable `answer` (for example `answer = 6`) in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [5ARB0_Analysis_3_5b] Finding an appropriate number of clusters\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [5ARB0_Analysis_3_5b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we find an appropriate number of clusters for representing some data set?\n",
    "\n",
    "1. We select the number of clusters where the loss function begins to increase.\n",
    "2. We can monitor the loss function and select the number of clusters for which it obtains its lowest value. The corresponding number of clusters is an appropriate number of clusters.\n",
    "3. We could add a penalty term for the number of clusters to the cost and then select the lowest value.\n",
    "4. All of the above answers are correct.\n",
    "5. None of the above answers are correct\n",
    "\n",
    "Answer this question by assigning the number corresponding to your answer to the variable `answer` (for example `answer = 6`) in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [5ARB0_Analysis_3_5c] how can we find the number of clusters\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [5ARB0_Analysis_3_5c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 3.5\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 3.6: Shortcoming of the K-means algorithm\n",
    "Apply the Kmeans algorithm for the new data set generate by `X = ex36_generate_data()`. Visualise the data and come up with an appropriate number of clusters. Plot the data points in a scatter plot, plot the means as red crosses in the same plot and color the data point according to their assigned cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data\n",
    "X = ex36_generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [5ARB0_Analysis_3_6a] Plot clusters of new data set\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [5ARB0_Analysis_3_6a]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 3.6\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Gaussian Mixture modeling\n",
    "The Kmeans algorithm is a very fast and simple algorithm that works well for many applications. However, because of its simplicity it is not suitable for all situations, such as the one described in exercise 1.6. In this part we present another methodology for clustering data, namely through Gaussian mixture modeling. In this approach we do not rely on a deterministic algorithm for determining the cluster means and assignments, but instead we model the data set by a probability density function.\n",
    "\n",
    "We will assume that the data set has been generated from a Gaussian mixture model, which is formally specified as\n",
    "$$ p({\\bf{x}}^{(n)}) = \\sum_{k=1}^K \\rho_k \\mathcal{N}({\\bf{x}}^{(n)} \\mid {\\bf{\\mu}}_k, \\Sigma_k),$$\n",
    "where a data sample ${\\bf{x}}^{(n)}$ is originating from a Gaussian mixture model with $K$ individual Gaussian distributions with means ${\\bf{\\mu}}_k$ and covariance matrices ${\\bf{\\Sigma}}_k$. The mean denotes the center or mode of the Gaussian distribution and the covariance matrix specifies the strech and tilt of the Gaussian distribution. In this model the mixing coefficients $\\rho_k$ specify how much each of the Gaussian distributions contributes in the model. Because the Gaussian mixture model is a probability density function, integrating over ${\\bf{x}}$ should always equal 1. Because the individual Gaussians already satisfy this requirement, the mixing coefficients are constrained by\n",
    "$$ \\sum_{k=1}^K \\rho_k = 1.$$\n",
    "To give some intuition on this model, we give a 1-dimensional example below. Here we model a data set by a mixture of 2 Gaussians. The individual *weighted* Gaussian distributions are colored in blue and the corresponding mixture model distribution is colored in red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex4_plot_GMM_1D()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During this part of the assignment you will implement the so-called Expectation-Maximization (EM) algorithm for learning the Gaussian mixture model. This algorithm consists of two step, the expectation step (E-step) and the maximization step (M-step). The exact details of the algorithm are beyond the scope of this assignment, but here we will present the update equations for these steps.\n",
    "\n",
    "The EM algorithm works as follows:\n",
    "\n",
    "1. Initialize the means ${\\bf{\\mu}}_k$, covariances $\\Sigma_k$ and mixing coefficients $\\rho_k$. Often the means are initialized using the Kmeans algorithm. The covariance matrices can be set to identity matrices and the mixing coefficients can be initialized to the fraction of points assigned to the cluster with Kmeans divided by the total number of samples.\n",
    "2. *Expectation step*: evaluate the responsibilities $\\gamma_{nk}$ using the current parameter values as \n",
    "$$ \\gamma_{nk} = \\frac{\\rho_k \\mathcal{N}({\\bf{x}}_n \\mid {\\bf{\\mu}}_k, \\Sigma_k)}{\\sum_{j=1}^K \\rho_j \\mathcal{N}({\\bf{x}}_n \\mid {\\bf{\\mu}}_j, \\Sigma_j)}$$\n",
    "3. *Maximization step*: re-estimate the parameters using the current responsibilities\n",
    "$$ {\\bf{\\mu}}_k^\\text{new} = \\frac{1}{N_k} \\sum_{n=1}^N \\gamma_{nk}{\\bf{x}}_n $$\n",
    "$$ \\Sigma_k^\\text{new} = \\frac{1}{N_k} \\sum_{n=1}^N \\gamma_{nk} ({\\bf{x}}_n - {\\bf{\\mu}}_k^\\text{new})({\\bf{x}}_n - {\\bf{\\mu}}_k^\\text{new})^\\top $$\n",
    "$$ \\rho_k = \\frac{N_k}{N} $$\n",
    "where $N$ denotes the number of samples and where\n",
    "$$ N_k = \\sum_{n=1}^N \\gamma_{nk}$$\n",
    "4. Evaluate the log-likelihood\n",
    "$$ \\ln p({\\bf{X}} \\mid {\\bf{\\mu}}, \\Sigma, {\\bf{\\rho}}) = \\sum_{n=1}^N \\ln \\left\\{ \\sum_{k=1}^K \\rho_k \\mathcal{N}({\\bf{x}}^{(n)} \\mid {\\bf{\\mu}}_k, \\Sigma_k)\\right\\}$$\n",
    "\n",
    "It is important to grasp what is going on in this algorithm. The responsibilities $\\gamma_{nk}$ are similar to the indicator functions from the Kmeans algorithm. However, where the Kmeans algorithm performs a hard clustering (each point can be assigned to only 1 cluster), the Gaussian mixture model allows for a soft clustering (each point can be modeled by both Gaussian distribution, but just to a different extent). The indicator function of the Kmeans algorithm was one-hot coded, meaning that a point was assigned to 1 cluster only. The responsibilities $\\gamma_{nk}$ specify how likely a data sample ${\\bf{x}}_n$ is to be generated from a cluster. With a Gaussian mixture model a point can therefore be assigned to different extents to multiple clusters. The expectation step calculates these responsibilities and the division in this expression makes sure that all rows sum op to 1.\n",
    "\n",
    "In the maximization step the parameters are updated. Here the contribution of each data sample towards the parameters depends on the corresponding responsibilities. This means that a point that is very likely to have originated from a certain cluster will have a high influence on the statistics of that cluster. The variable $N_k$ specifies how many points are located to a certain Gaussian distribution. Because this parameter is the summation over the individual responsibilities, $N_k$ is not forced to be an integer.\n",
    "\n",
    "The log-likelihood is a cost function which takes the variances and uncertainties in our model into account. It describes the probability of that data set being generated from a Gaussian mixture model. To prevent numerical instability we commonly use the log-likelihood instead of the normal likelihood.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 4.1: Initialize clusters\n",
    "Consider the function from the previous part `X = ex46_generate_data()` which generates a matrix ${\\bf{X}}$ of shape (N x D), representing the vertical concatenation of $N$ transposed data vectors of dimension $D$. Create a function `means, covs, rho = initialize_GMM(X, K)` that accepts the data set ${\\bf{X}}$ as input and returns the following in this order:\n",
    "- `means`: a matrix of size (K x D) that contains the initial cluster means, as a vertical concatenation of the transposed mean vectors. These means should be initialized using the previously written K-means algorithm.\n",
    "- `covs`: a matrix of size (K x D x D) that contains the covariance matrices of the initial clusters. Each matrix `covs[k,:,:]` represents the covariance matrix of the $k^\\text{th}$ cluster. Initialize these covariance matrices as identity matrices.\n",
    "- `rho`: a vector of length K that contains the mixing coefficients as specified above. Initialize this vector based on the indicator function returned by the K-means algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [5ARB0_Analysis_4_1] Complete the initialize_GMM(X, K) function\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [5ARB0_Analysis_4_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch data\n",
    "X = ex36_generate_data()\n",
    "\n",
    "# initialize GMM\n",
    "means, covs, rho = initialize_GMM(X, 2)\n",
    "\n",
    "# plot GMM\n",
    "ex4_plot_GMM(X, means, covs, rho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 4.1\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 4.2: Expectation step\n",
    "Create a function `gamma = expectation_step(X, means, covs, rho)` that accepts the data set, means, covariances and mixing coefficients with dimensions specified above. This function should perform the expectation step and should return the calculated responsibilities as defined above as a matrix of size (N x K) where each row corresponds to the assignment fraction of a sample amongst the different clusters. Make sure this matrix is properly normalized such that the elements in each row add up to 1. Use the `multivariate_normal` function that has been imported from `scipy.stats` at the beginning of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [5ARB0_Analysis_4_2] Complete the expectation_step(X, means, covs, rho) function\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [5ARB0_Analysis_4_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = expectation_step(X, means, covs, rho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 4.2\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 4.3: Maximization step\n",
    "Create a function `means, covs, rho = maximization_step(X, gamma)` that accepts the data set and responsibilities with dimensions specified above. This function should perform the maximization step and should return the new means, covariances and mixing coefficients with dimensions as specified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [5ARB0_Analysis_4_3] Complete the maximization_step(X, gamma) function\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [5ARB0_Analysis_4_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximization step\n",
    "means, covs, rho = maximization_step(X, gamma)\n",
    "\n",
    "# plot GMM\n",
    "ex4_plot_GMM(X, means, covs, rho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 4.3\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 4.4: Log-likelihood calculation\n",
    "Create a function `J = loglikelihood(X, means, covs, rho)` that accepts the data set, means, covariance matrices and mixing coefficients with dimensions specified above. This function should calculate and return the log-likelihood of the data under the specified Gaussian mixture model. Use the definition as specified in the beginning of Part 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [5ARB0_Analysis_4_4] Complete the loglikelihood(X, means, covs, rho) function\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [5ARB0_Analysis_4_4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 4.4\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 4.5: Gaussian mixture modeling\n",
    "Now that all the subfunctions have been defined it is time to tie them together and to form a function which does the Gaussian mixture modelling. Create a function `means, covs, rho, gamma, J = GMM_modeling(X, K, nr_iterations)` that does the following:\n",
    "\n",
    "1. Initialize the parameters of the Gaussian mixture model.\n",
    "2. Performs `nr_iterations` iterations of the following:\n",
    "    1. Perform the expectation step.\n",
    "    2. Perform the maximization step.\n",
    "    3. Calculate the log-likelihood.\n",
    "3. returns the parameters and a vector of saved values of the log-likelihood.\n",
    "\n",
    "The function should return all the parameters of the trained Gaussian mixture model, containing the final means, covariance matrices, mixing coefficients, responsibilities and a vector containing all calculated values of the log-likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [5ARB0_Analysis_4_5] Complete the GMM_modeling(X, K, nr_iterations) function\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [5ARB0_Analysis_4_5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train GMM\n",
    "means, covs, rho, gamma, J = GMM_modeling(X, 2, 10)\n",
    "\n",
    "# plot GMM\n",
    "ex4_plot_GMM(X, means, covs, rho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(J)\n",
    "plt.grid(), plt.xlabel(\"iteration\"), plt.ylabel(\"log-likelihood\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 4.5\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">   Make sure to restart this notebook and to rerun all cells before submission to check whether all code runs properly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
